{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import utils.model_helper as mh\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = '/home/irizqy/ml_ws/bangkit-ws/data/bizz.it-sim_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path_arr = os.listdir(DATASET_PATH)\n",
    "im_classes_arr = []\n",
    "\n",
    "for file in im_path_arr:\n",
    "    im_classes_arr.append(file.split('_')[0])\n",
    "\n",
    "im_classes_arr = np.asarray(im_classes_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(im_classes_arr)\n",
    "\n",
    "dict_keys = {val:key for key, val in enumerate(classes.flatten())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_im_path = [np.where(im_classes_arr == cls)[0] for cls in classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_pairs(ims_arr, percentage):\n",
    "    train_pairs = []\n",
    "    train_labels = []\n",
    "    test_pairs = []\n",
    "    test_labels = []\n",
    "\n",
    "    for file in ims_arr:\n",
    "        file_cls = file.split('_')[0]\n",
    "        cls = dict_keys[file_cls]\n",
    "\n",
    "        current_im = cv.imread(os.path.join(DATASET_PATH, file))\n",
    "        current_im = cv.cvtColor(current_im, cv.COLOR_BGR2RGB)\n",
    "        # current_im = cv.cvtColor(current_im, cv.COLOR_BGR2GRAY)\n",
    "        current_im = cv.resize(current_im, (150, 150))\n",
    "        current_im = current_im / 255\n",
    "\n",
    "        pos_idx = np.random.choice(grouped_im_path[cls], 1)[0]\n",
    "        pos_pair_im = cv.imread(os.path.join(DATASET_PATH, im_path_arr[pos_idx]))\n",
    "        pos_pair_im = cv.cvtColor(pos_pair_im, cv.COLOR_BGR2RGB)\n",
    "        # pos_pair_im = cv.cvtColor(pos_pair_im, cv.COLOR_BGR2GRAY)\n",
    "        pos_pair_im = cv.resize(pos_pair_im, (150, 150))\n",
    "        pos_pair_im = pos_pair_im / 255\n",
    "\n",
    "        train_pairs.append((current_im, pos_pair_im))\n",
    "        train_labels.append(1)\n",
    "\n",
    "        neg_idx = np.random.choice(np.where(im_classes_arr != file_cls)[0], 1)[0]\n",
    "        neg_pair_im = cv.imread(os.path.join(DATASET_PATH, im_path_arr[neg_idx]))\n",
    "        neg_pair_im = cv.cvtColor(neg_pair_im, cv.COLOR_BGR2RGB)\n",
    "        # neg_pair_im = cv.cvtColor(neg_pair_im, cv.COLOR_BGR2GRAY)\n",
    "        neg_pair_im = cv.resize(neg_pair_im, (150, 150))\n",
    "        neg_pair_im = neg_pair_im / 255\n",
    "\n",
    "        train_pairs.append((current_im, neg_pair_im))\n",
    "        train_labels.append(0)\n",
    "\n",
    "    arr_length = len(train_pairs)\n",
    "    num_of_data = int(percentage * arr_length)\n",
    "\n",
    "    for i in range(num_of_data):\n",
    "        rand_i = np.random.randint(arr_length - 1)\n",
    "        test_pairs.append(train_pairs[rand_i])\n",
    "        train_pairs.pop(rand_i)\n",
    "        test_labels.append(train_labels[rand_i])\n",
    "        train_labels.pop(rand_i)\n",
    "\n",
    "        arr_length -= 1\n",
    "\n",
    "    return np.asarray(train_pairs), np.asarray(train_labels), np.asarray(test_pairs), np.asarray(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs, train_labels, test_pairs, test_labels = make_train_test_pairs(im_path_arr, .2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 20\n",
    "h = 15\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "columns = 8\n",
    "rows = 8\n",
    "for index, i in enumerate(range(1, (columns*rows +1)//2)):\n",
    "    img = train_pairs[index][0]\n",
    "    fig.add_subplot(rows, columns, 2*i - 1) \n",
    "    plt.title(train_labels[index])\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    img = train_pairs[index][1]\n",
    "    fig.add_subplot(rows, columns, 2*i)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (150, 150, 3)\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseModel:\n",
    "\n",
    "    def __init__(self, input_shape, embedding_dim=224):\n",
    "        self.input_shape = input_shape\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "    def _build(self):\n",
    "        inputs = tf.keras.layers.Input(self.input_shape)\n",
    "        \n",
    "        x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(inputs)\n",
    "        x = tf.keras.layers.MaxPooling2D()(x)\n",
    "        x = tf.keras.layers.Dropout(.1)(x)\n",
    "\n",
    "        x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "        x = tf.keras.layers.MaxPooling2D()(x)\n",
    "\n",
    "        # x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
    "        # x = tf.keras.layers.MaxPooling2D()(x)\n",
    "\n",
    "        # x = tf.keras.layers.Conv2D(16, (3, 3), activation='relu')(x)\n",
    "        # x = tf.keras.layers.MaxPooling2D()(x)\n",
    "        # x = tf.keras.layers.Dropout(.3)(x)\n",
    "\n",
    "        # pooled_output = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        pooled_output = tf.keras.layers.Flatten()(x)\n",
    "        outputs = tf.keras.layers.Dense(self.embedding_dim)(pooled_output)\n",
    "\n",
    "        model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm = SiameseModel(IMG_SHAPE)\n",
    "\n",
    "# featureExtractor = sm._build()\n",
    "# featureExtractor.summary()\n",
    "\n",
    "base_cnn =  tf.keras.applications.resnet.ResNet50(\n",
    "    weights='imagenet', input_shape=IMG_SHAPE, include_top=False\n",
    ")\n",
    "\n",
    "cnn_model = base_cnn.get_layer('conv5_block3_2_conv')\n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(cnn_model.output)\n",
    "# dense_1 = tf.keras.layers.Dense(units=256, activation='relu')(flatten)\n",
    "dense_2 = tf.keras.layers.Dense(units=128, activation='relu')(flatten)\n",
    "\n",
    "featureExtractor = tf.keras.Model(base_cnn.input, dense_2)\n",
    "\n",
    "for layer in featureExtractor.layers[:-15]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureExtractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vectors):\n",
    "\t# unpack the vectors into separate lists\n",
    "\t(feats_A, feats_B) = vectors\n",
    "\t# compute the sum of squared distances between the vectors\n",
    "\tsum_squared = K.sum(K.square(feats_A - feats_B), axis=1,\n",
    "\t\tkeepdims=True)\n",
    "\t# return the euclidean distance between the vectors\n",
    "\treturn K.sqrt(K.maximum(sum_squared, K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vectors):\n",
    "    (featsA, featsB) = vectors\n",
    "    sum_product = K.sum(featsA*featsB)\n",
    "    sum_squared_featsA = K.sqrt(K.sum(featsA**2, keepdims=1, axis=1))\n",
    "    sum_squared_featsB = K.sqrt(K.sum(featsB**2, keepdims=1, axis=1))\n",
    "    sum_mul_feats = sum_squared_featsA * sum_squared_featsB\n",
    "\n",
    "    return sum_product / sum_mul_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure the siamese network\n",
    "print(\"[INFO] building siamese network...\")\n",
    "imgA = tf.keras.layers.Input(shape=IMG_SHAPE)\n",
    "imgB = tf.keras.layers.Input(shape=IMG_SHAPE)\n",
    "featsA = featureExtractor(imgA)\n",
    "featsB = featureExtractor(imgB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, construct the siamese network\n",
    "distance = tf.keras.layers.Lambda(euclidean_distance)([featsA, featsB])\n",
    "outputs = tf.keras.layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "model = tf.keras.Model(inputs=[imgA, imgB], outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# compile the model\n",
    "print(\"[INFO] compiling model...\")\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=tf.keras.optimizers.Adam(),\n",
    "\tmetrics=[\"accuracy\"])\n",
    "# train the model\n",
    "print(\"[INFO] training model...\")\n",
    "history = model.fit(\n",
    "\t[train_pairs[:, 0], train_pairs[:, 1]], train_labels[:],\n",
    "\tvalidation_data=([test_pairs[:, 0], test_pairs[:, 1]], test_labels[:]),\n",
    "\tbatch_size=BATCH_SIZE, \n",
    "\tepochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(H):\n",
    "\t# construct a plot that plots and saves the training history\n",
    "\tplt.style.use(\"ggplot\")\n",
    "\tplt.figure()\n",
    "\tplt.plot(H.history[\"loss\"], label=\"train_loss\")\n",
    "\tplt.plot(H.history[\"val_loss\"], label=\"val_loss\")\n",
    "\tplt.plot(H.history[\"accuracy\"], label=\"train_acc\")\n",
    "\tplt.plot(H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "\tplt.title(\"Training Loss and Accuracy\")\n",
    "\tplt.xlabel(\"Epoch #\")\n",
    "\tplt.ylabel(\"Loss/Accuracy\")\n",
    "\tplt.legend(loc=\"lower left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/home/irizqy/ml_ws/bangkit-ws/src/logo-detector/im_similar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 00:38:55.141804: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-06-08 00:38:55.142394: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('/home/irizqy/ml_ws/bangkit-ws/src/logo-detector/im_similar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 71ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0017043775"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image, ImageOps\n",
    "\n",
    "preds = []\n",
    "\n",
    "test_data = os.listdir('/home/irizqy/ml_ws/bangkit-ws/data/bizz.it-sim_dataset')\n",
    "\n",
    "path_im1_1 = '/home/irizqy/ml_ws/bangkit-ws/src/logo-detector/cropped-logo0.jpg'\n",
    "path_im1_2 = '/home/irizqy/Downloads/yamie-panda.jpeg'\n",
    "\n",
    "path_im2_1 = '/home/irizqy/ml_ws/bangkit-ws/data/bizz.it-sim_dataset/sabana_12.jpg'\n",
    "path_im2_2 = '/home/irizqy/Downloads/Screenshot from 2023-06-08 00-18-31.png'\n",
    "\n",
    "im1 = mh.adjust_im(path_im1_2, (150, 150))\n",
    "im2 = mh.adjust_im(path_im2_2, (150, 150))\n",
    "\n",
    "model.predict((im1, im2))[0][0]\n",
    "\n",
    "# for file in test_data:\n",
    "#     im2 = cv.imread(os.path.join('/home/irizqy/ml_ws/bangkit-ws/data/bizz.it-sim_dataset', file))\n",
    "#     print(file)\n",
    "#     im2 = cv.resize(im2, (150, 150))\n",
    "#     im2 = cv.cvtColor(im2, cv.COLOR_BGR2RGB)\n",
    "\n",
    "#     im2 = im2 / 255\n",
    "\n",
    "#     image2 = np.expand_dims(im2, 0)\n",
    "#     pred = model.predict((im1, image2))[0][0]\n",
    "#     preds.append(pred)\n",
    "\n",
    "# preds = np.asarray(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_idx = np.argmax(preds)\n",
    "print(max_idx, preds[max_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[max_idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bangkit-ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
